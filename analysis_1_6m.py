# -*- coding: utf-8 -*-
"""Analysis_1.6M.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CoL7XUv6fHkCjL3uoLKhsXgpYIgNaq2U
"""

import pandas as pd
import re
import nltk
import matplotlib.pyplot as plt
import json

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Geetansh/Data Literacy/sentiment-ChatGPT/data/chatgpt_tweets_latest_cured.csv")
pd.set_option('display.max_colwidth', None)

df.head(5)

df.shape

df.keys()

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

stoplist = set(stopwords.words("english"))
stoplist.remove('not')

df['text'] = df['text'].fillna('').apply(str)

def preprocess_text(df):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    df["processed_text"] = df["text"].apply(lambda x: re.sub(r'https?:\/\/\S+', '', x))
    df["processed_text"] = df["processed_text"].apply(lambda x: re.sub(r'@\S+', '', x))
    df["processed_text"] = df["processed_text"].apply(lambda x: x.lower())
    df["processed_text"] = df["processed_text"].apply(lambda x: re.sub(r'[^\w\s]', '', x))
    df["processed_text"] = df["processed_text"].apply(lambda x: word_tokenize(x))
    df["processed_text"] = df["processed_text"].apply(lambda x: [word for word in x if word not in stoplist])
    return df

processed_df = preprocess_text(df)

processed_df.head(5)

from keras.models import load_model
from joblib import load
from keras_preprocessing.sequence import pad_sequences

# Load the pre-trained model and tokenizer
model = load_model("/content/drive/MyDrive/Geetansh/Data Literacy/sentiment-ChatGPT/model/model.h5")
tokenizer = load("/content/drive/MyDrive/Geetansh/Data Literacy/sentiment-ChatGPT/model/tokenizer.pkl")

# Define constants
SEQUENCE_LENGTH = 300
SENTIMENT_THRESHOLDS = (0.4, 0.7)

def predict(text):
    # Use the tokenizer to convert the text into a sequence
    sequence = tokenizer.texts_to_sequences([text])
    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)

    # Use the model to make a prediction
    prediction = model.predict(sequence)
    score = prediction[0][0]

    # Decode the sentiment
    if score < SENTIMENT_THRESHOLDS[0]:
        label = "NEGATIVE"
    elif score < SENTIMENT_THRESHOLDS[1]:
        label = "NEUTRAL"
    else:
        label = "POSITIVE"

    return {"label": label, "score": score}

# Example usage
print(predict("Lalitha should like vegetables"))

# Add a new column 'sentiment' to the DataFrame

with tf.device('/device:GPU:0'):
    processed_df['sentiment'] = processed_df['processed_text'].apply(predict)

processed_df.head()

processed_df['sentiment_label'] = processed_df['sentiment'].apply(lambda x: x['label'])
processed_df['sentiment_score'] = processed_df['sentiment'].apply(lambda x: x['score'])

# Save the updated DataFrame to a CSV file
processed_df.to_csv("/content/drive/MyDrive/Geetansh/Data Literacy/sentiment-ChatGPT/data/processed_data.csv", index=False)

processed_df = pd.read_csv("/content/drive/MyDrive/Geetansh/Data Literacy/sentiment-ChatGPT/data/processed_data_.csv", low_memory=False)

processed_df.columns

processed_df

# Count the number of instances of each sentiment label
sentiment_counts = processed_df['sentiment_label'].value_counts()

# Create a pie chart
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%.1f%%')
plt.title("Sentiment Distribution")

# Display the chart
plt.show()

!pip install plotly

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.io as pio
pio.renderers.default = 'colab'
import plotly.graph_objects as go

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:100% !important; }</style>"))

sentiment_counts = processed_df['sentiment_label'].value_counts()
trace = go.Pie(labels=sentiment_counts.index, values=sentiment_counts, textinfo='percent')
layout = go.Layout(title="Sentiment Distribution")
data = [trace]
fig = go.Figure(data=data, layout=layout)
py.iplot(fig)

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.io as pio
pio.renderers.default = 'colab'
import plotly.graph_objects as go
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:50% !important; }</style>"))
sentiment_counts = processed_df['sentiment_label'].value_counts()
trace = go.Pie(labels=sentiment_counts.index, values=sentiment_counts, textinfo='label+value+percent')
layout = go.Layout(title="Sentiment Distribution - 1.6M", width=500, height=500)
data = [trace]
fig = go.Figure(data=data, layout=layout)
py.iplot(fig)

import matplotlib.pyplot as plt

# Create a new DataFrame with the count of each sentiment label
sentiment_count = processed_df['sentiment_label'].value_counts().reset_index()
sentiment_count.columns = ['sentiment_label', 'count']

# Create the bar chart
plt.bar(sentiment_count['sentiment_label'], sentiment_count['count'])
plt.xlabel('Sentiment Label')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.show()

# processed_df = processed_df.drop(index=23375)

# Convert date column to datetime format
processed_df['date'] = pd.to_datetime(processed_df['date'], errors = 'raise')

# Remove the timezone offset
processed_df['date'] = processed_df['date'].dt.tz_localize(None)

# Extract date only
processed_df['date'] = processed_df['date'].dt.date
processed_df['date'].unique

daily_sentiment = processed_df.groupby(['date','sentiment_label']).size().reset_index(name='counts')

# Use pivot_table to reshape the DataFrame 
daily_sentiment = daily_sentiment.pivot(index='date',columns='sentiment_label',values='counts')

# Fill missing values with 0
daily_sentiment = daily_sentiment.fillna(0)

# Create a new figure and subplots
fig, ax = plt.subplots(figsize=(20, 10))

# Plot the daily sentiment distribution
daily_sentiment.plot(kind='line',ax=ax,linewidth=0.5)
plt.title("Daily Sentiment Distribution",fontsize=20)
plt.xlabel("Date",fontsize=20)
plt.ylabel("Count",fontsize=20)
plt.show()

processed_df.keys()

# Create a new DataFrame with the count of each sentiment label for each day
daily_sentiment = processed_df.groupby(['date','sentiment_label']).size().reset_index(name='counts')

# Use pivot_table to reshape the DataFrame 
daily_sentiment = daily_sentiment.pivot(index='date',columns='sentiment_label',values='counts')

# Plot the daily sentiment distribution
fig, ax = plt.subplots(figsize=(20,10))

daily_sentiment.plot(kind='bar', stacked=True, ax=ax)
plt.title("Daily Sentiment Distribution")
plt.xlabel("Date")
plt.ylabel("Count")
plt.show()

# Count the number of tweets for each day
daily_tweets = processed_df.groupby(['date']).size().reset_index(name='counts')

# Plot the daily tweet counts
plt.figure(figsize=(20,10))

plt.bar(daily_tweets['date'], daily_tweets['counts'])
plt.title("Daily Tweet Counts")
plt.xlabel("Date")
plt.ylabel("Count")
plt.show()

import math

for i, value in enumerate(processed_df['processed_text']):
    if isinstance(value, float) and math.isnan(value):
        processed_df.at[i, 'processed_text'] = ""

processed_df['processed_text'] = processed_df['processed_text'].apply(lambda x: x.replace('chatgpt',''))
processed_text = [word for word in processed_df['processed_text'] if word.isalnum() and len(word) > 1]

processed_df





import numpy as np

# Group the tweets by date
date_group = processed_df.groupby(['date','sentiment_label']).size().reset_index(name='counts')
date_sentiment = pd.pivot_table(date_group, values='counts', index=['date'], columns=['sentiment_label'], aggfunc=np.sum, fill_value=0)
date_sentiment = date_sentiment.div(date_sentiment.sum(axis=1), axis=0)*100

plt.style.use('ggplot')

fig = plt.figure(figsize=(15,7))
for sentiment in date_sentiment.columns:
    plt.plot(date_sentiment.index, date_sentiment[sentiment], label=sentiment)
plt.xlabel('Date')
plt.ylabel('Sentiment Percentage')
plt.title('Sentiment Percentage over Time')
plt.legend()
plt.show()

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

# Create a string of all the words in the 'processed_text' column
text = " ".join(review for review in processed_df.processed_text)

# Create a word cloud with the text and set the background color to white
wordcloud = WordCloud(stopwords=STOPWORDS, background_color="white", max_words=100, width=1920, height=1080, contour_width=3, contour_color='steelblue').generate(text)

# Display the word cloud
plt.figure(figsize=(20,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

from nltk.probability import FreqDist
import re

# Join all the text in processed_df['processed_text']
all_text = " ".join(text for text in processed_df['processed_text'])
all_text = re.sub("[^a-zA-Z]", " ", all_text)
all_text = all_text.lower()

# Tokenize the text
tokens = nltk.word_tokenize(all_text)
print(tokens[0:20])

# Generate the frequency distribution
fdist = FreqDist(tokens)

# Plot the frequency distribution
plt.figure(figsize=(20, 20))
fdist.plot(30, cumulative=False)
plt.show()

positive_df = processed_df[processed_df['sentiment_label'] == 'POSITIVE']
positive_words = " ".join(review for review in positive_df["processed_text"])

# Create a string of all the words in the 'processed_text' column
text = " ".join(review for review in processed_df.processed_text)

# Create a word cloud with the text and set the background color to white
wordcloud = WordCloud(stopwords=STOPWORDS, background_color="white", max_words=100, width=1920, height=1080, contour_width=3, contour_color='steelblue').generate(text)

# Display the word cloud
plt.figure(figsize=(20,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Join all the text
all_text = " ".join(text for text in positive_df['processed_text'])
all_text = re.sub("[^a-zA-Z]", " ", all_text)
all_text = all_text.lower()

# Tokenize the text
tokens = nltk.word_tokenize(all_text)
print(tokens[0:20])

# Generate the frequency distribution
fdist = FreqDist(tokens)

# Plot the frequency distribution
plt.figure(figsize=(10, 10))
fdist.plot(30, cumulative=False)
plt.show()

negative_df = processed_df[processed_df['sentiment_label'] == 'NEGATIVE']
negative_words = " ".join(review for review in negative_df["processed_text"])

# Create a string of all the words in the 'processed_text' column
text = " ".join(review for review in processed_df.processed_text)

# Create a word cloud with the text and set the background color to white
wordcloud = WordCloud(stopwords=STOPWORDS, background_color="white", max_words=100, width=1920, height=1080, contour_width=3, contour_color='steelblue').generate(text)

# Display the word cloud
plt.figure(figsize=(20,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Join all the text
all_text = " ".join(text for text in negative_df['processed_text'])
all_text = re.sub("[^a-zA-Z]", " ", all_text)
all_text = all_text.lower()

# Tokenize the text
tokens = nltk.word_tokenize(all_text)
print(tokens[0:20])

# Generate the frequency distribution
fdist = FreqDist(tokens)

# Plot the frequency distribution
plt.figure(figsize=(20, 20))
fdist.plot(30, cumulative=False)
plt.show()

tech_keywords = ["ai", "machine learning", "tech", "it", "i.t.", "computer","programmer", "coder", "techie", "machine learning", "machine learning engineer", "data scientist", "data analyst", "data engineer", "AI", "artificial intelligence", "deep learning", "neural networks", "computer vision", "natural language processing", "big data", "cloud computing", "software developer", "web", "developer", "front-end", "back-end", "full-stack", "python", "r", "java", "c++", "javascript", "sql", "algorithms", "devops", "linux", "git", "agile", "scrum", "intern", "google", "microsoft", "amazon", "facebook", "apple", "open source", "hackathon", "startup", "entrepreneur", "research", "ph.d.", "masters", "bachelor", "student", "engineer", "consultant", "analyst", "manager", "director", "lead", "senior", "junior", "summer of code", "github", "stack overflow", "kaggle", "coursera", "edx", "udemy", "codecademy", "khan academy", "mit opencourseware", "stanford online", "javascript", "python", "c++", "java", "php", "sql", "git", "linux", "windows", "mac", "devops", "agile", "scrum", "kanban", "software development", "full stack", "front-end", "back-end", "api", "data science", "big data", "data engineering", "data analyst", "data visualization", "neural networks", "deep learning", "computer vision", "natural language processing", "reinforcement", "learning", "computer science", "computer engineer", "software engineer", "programmer", "coder", "developer", "debugging", "code review", "algorithm", "data structure", "cloud computing", "aws", "azure", "google cloud", "ibm cloud", "oracle", "cloud", "open source", "docker", "kubernetes", "ai engineer", "machine learning engineer", "data engineer", "data scientist", "ai researcher"]

processed_df['user_description']

processed_df['user_description_processed'] = processed_df['user_description'].apply(lambda x: x if isinstance(x, str) else '')

def preprocess_text(text):
    text = text.lower() # convert to lowercase
    text = re.sub(r'https?:\/\/\S+', '', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text) # remove emojis and other symbols
    text = text.split() # break into tokens
    return text

processed_df['user_description_processed'] = processed_df['user_description_processed'].apply(preprocess_text)

processed_df

tech_df = processed_df[processed_df['user_description_processed'].apply(lambda x: any(word in x for word in tech_keywords))]

tech_df

# Create a new DataFrame with the count of each sentiment label
sentiment_count = tech_df['sentiment_label'].value_counts().reset_index()
sentiment_count.columns = ['sentiment_label', 'count']

# Create the bar chart
plt.bar(sentiment_count['sentiment_label'], sentiment_count['count'])
plt.xlabel('Sentiment Label')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.show()

sentiment_counts = tech_df['sentiment_label'].value_counts()

# Create a pie chart
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%.1f%%')
plt.title("Sentiment Distribution")

# Display the chart
plt.show()

import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)
  
# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

